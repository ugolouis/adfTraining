{
	"name": "8_CDS_EmergencyVisitData_Main_Reconciliation_sp",
	"properties": {
		"folder": {
			"name": "ECDS_prod_notebooks"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "ce32fbbf-0aad-43f3-b5b7-c570382cb88e"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# CDS_EmergencyVisitData_Main_Reconciliation_sp"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Version 1\r\n",
					"\r\n",
					"Delta Lake 0.8+"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import col\r\n",
					"from delta.tables import DeltaTable\r\n",
					"\r\n",
					"# Define paths\r\n",
					"storage_account_name = \"synapsestr\"\r\n",
					"delta_container_name = \"nervecentre-delta\"\r\n",
					"main_table_path = f'abfss://{delta_container_name}@{storage_account_name}.dfs.core.windows.net/ECDS_EmergencyVisitData_Main'\r\n",
					"history_table_path = f'abfss://{delta_container_name}@{storage_account_name}.dfs.core.windows.net/SPB/PatientVisitHistoryData_Current'\r\n",
					"\r\n",
					"# Read tables\r\n",
					"main_table = DeltaTable.forPath(spark, main_table_path)\r\n",
					"history_df = spark.read.format(\"delta\").load(history_table_path)\r\n",
					"\r\n",
					"# Identify records to delete\r\n",
					"to_delete = main_table.toDF().join(\r\n",
					"    history_df.select(\"visitarchiveid\"),\r\n",
					"    \"visitarchiveid\",\r\n",
					"    \"left_anti\"\r\n",
					")\r\n",
					"\r\n",
					"# Perform the delete operation\r\n",
					"main_table.delete(condition=col(\"visitarchiveid\").isin(to_delete.select(\"visitarchiveid\")))\r\n",
					"\r\n",
					"# Log the number of deleted rows (optional)\r\n",
					"deleted_rows = to_delete.count()\r\n",
					"print(f\"Number of rows deleted: {deleted_rows}\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Version 2\r\n",
					"\r\n",
					"Earlier version of Delta Lke  "
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.functions import col\r\n",
					"from delta.tables import DeltaTable\r\n",
					"\r\n",
					"# Define paths\r\n",
					"storage_account_name = \"synapsestr\"\r\n",
					"delta_container_name = \"nervecentre-delta\"\r\n",
					"main_table_path = f'abfss://{delta_container_name}@{storage_account_name}.dfs.core.windows.net/ECDS_EmergencyVisitData_Main'\r\n",
					"history_table_path = f'abfss://{delta_container_name}@{storage_account_name}.dfs.core.windows.net/SPB/PatientVisitHistoryData_Current'\r\n",
					"\r\n",
					"# Read tables\r\n",
					"main_table = DeltaTable.forPath(spark, main_table_path)\r\n",
					"main_df = main_table.toDF()\r\n",
					"history_df = spark.read.format(\"delta\").load(history_table_path)\r\n",
					"\r\n",
					"# Identify records to keep\r\n",
					"to_keep = main_df.join(history_df.select(\"visitarchiveid\"), \"visitarchiveid\", \"inner\")\r\n",
					"\r\n",
					"# Overwrite the main table with records to keep\r\n",
					"to_keep.write.format(\"delta\").mode(\"overwrite\").save(main_table_path)\r\n",
					"\r\n",
					"# Log the number of deleted rows (optional)\r\n",
					"original_count = main_df.count()\r\n",
					"new_count = to_keep.count()\r\n",
					"deleted_rows = original_count - new_count\r\n",
					"print(f\"Number of rows deleted: {deleted_rows}\")"
				],
				"execution_count": null
			}
		]
	}
}